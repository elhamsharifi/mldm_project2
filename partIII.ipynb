{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part III: Ensembles and Final Result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import proj2_lib.util as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_pipeline_file': 'feature_pipeline.pkl',\n",
       " 'labels_pipeline_file': 'labels_pipeline.pkl',\n",
       " 'objstore_path': 'objects',\n",
       " 'processed_data_path': 'processed_data',\n",
       " 'raw_data_csv': 'KaggleV2-May-2016.csv',\n",
       " 'raw_data_path': 'data',\n",
       " 'test_csv': 'test_set.csv',\n",
       " 'train_csv': 'train_set.csv'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.file_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_config = utils.file_config\n",
    "#config['raw_data_path'] = \"some_other_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY NEED TO RUN THIS STEP ONCE (switch this to True to run it)\n",
    "RUN_MAKE_TRAIN_TEST_FILES = True\n",
    "if RUN_MAKE_TRAIN_TEST_FILES:\n",
    "    utils.make_train_test_sets(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import proj2_lib.preprocess as preprocess\n",
    "\n",
    "# ONLY NEED TO RUN THIS STEP ONCE\n",
    "RUN_FIT_PREPROCESSING = True\n",
    "if RUN_FIT_PREPROCESSING:\n",
    "    preprocess.fit_save_pipelines(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90514, 105)\n",
      "(90514,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X, test_y = preprocess.load_test_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 105)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "### Train an AdaBoost classifier using Decision Tree stubs as weak learners. Compare its performance to results obtained in Part II using 10 fold CV.\n",
    "### Here I train an AdaBoost Classifier on Decision Tree stubs (with max_depth=1). n_estimators=200 and algorithm=\"SAMME.R\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=1, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoost code goes here\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier (max_depth=1),n_estimators=200,algorithm=\"SAMME.R\", learning_rate=1)\n",
    "ada_clf.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we compute the AUC accuracy with using 10 fold cross validation. The mean of AUC accuracy is 73% which is higher than almost every model we have developed so far except the random forest. The score is the same as tunned random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1, n_estimators=200, random_state=None) Scores: AUC Mean: 0.73 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "roc_scores = cross_val_score(ada_clf, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "    \n",
    "print(\"clf\", ada_clf, \"Scores:\", \"AUC\",\"Mean: %0.2f (+/- %0.2f)\" % (roc_scores.mean(), roc_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Choose a set of 5 or so classifiers. Write a function that trains an ensemble using stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here  a function is written for training 5 models (Decision Tree, Random Forest, KNN, Linear SVC, AdaBoost) and then ensemble them with stacking and using LogisticRegression as the blending model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_stack_ensemble(X, y):\n",
    "    # create train/validation sets using StratifiedShuffleSplit\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2 , random_state=1234)\n",
    "    new_set=pd.DataFrame(X.copy())\n",
    "    new_set['label']=y.copy().ravel()\n",
    "\n",
    "    for train_index, test_index in split.split(new_set, new_set[\"label\"]):\n",
    "        new_train_set = new_set.iloc[train_index]\n",
    "        new_test_set = new_set.iloc[test_index]\n",
    "\n",
    "    new_train_y = np.array(pd.DataFrame(new_train_set['label'].copy(), columns=[\"label\"])).ravel()\n",
    "    new_train_X = np.array(new_train_set.drop('label', axis=1))\n",
    "\n",
    "    new_test_y = np.array(pd.DataFrame(new_test_set['label'].copy(), columns=[\"label\"])).ravel()\n",
    "    new_test_X = np.array(new_test_set.drop('label', axis=1))\n",
    "\n",
    "\n",
    "    # train classifiers in ensemble using train set\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    clf1 = DecisionTreeClassifier(max_depth=8)\n",
    "    clf1_fit=clf1.fit(new_train_X, new_train_y)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf2 = RandomForestClassifier(max_features= 18, n_estimators= 100)\n",
    "    clf2_fit=clf2.fit(new_train_X, new_train_y)\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier \n",
    "    clf3 = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf3_fit=clf3.fit(new_train_X, new_train_y)\n",
    "\n",
    "\n",
    "    from sklearn.svm import LinearSVC\n",
    "    clf4 = LinearSVC(dual=False, C= 0.125) # we use dual when number of features are much less than number of examples. in LinearSVC there is this option but if we use SVM with kernel='linear' there is no dual option. In this case the linearSvc is better than SVM with linaer Kernel in terms of running time.\n",
    "    clf4_fit=clf4.fit(new_train_X, new_train_y)\n",
    "\n",
    "    clf5 = AdaBoostClassifier(DecisionTreeClassifier (max_depth=1),n_estimators=200,algorithm=\"SAMME.R\", learning_rate=1)\n",
    "    clf5_fit=clf5.fit(new_train_X, new_train_y)    \n",
    "\n",
    "\n",
    "    # create new feature matrix for validation set by getting predictions from the ensemble classifiers\n",
    "\n",
    "    new_feature_matrix=np.full((new_test_X.shape[0],5), 0.0)\n",
    "    i=0\n",
    "    for clf in (clf1_fit,clf2_fit,clf3_fit,clf4_fit,clf5_fit):\n",
    "        new_feature_matrix[:,i]=clf.predict(new_test_X)\n",
    "        i+=1\n",
    "    \n",
    "    # train logistic regression classifier on new feature matrix\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    meta_clf=LogisticRegression()\n",
    "    blend_clf=meta_clf.fit(new_feature_matrix, new_test_y)\n",
    "    \n",
    "    # return all trained classifiers\n",
    "    return (clf1_fit,clf2_fit,clf3_fit,clf4_fit,clf5_fit,blend_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold cross validation to measure performance of your stacked classifier. See Part II solution to see how to roll your own sklearn classifier along with http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here Stack_Ensemble_Classifier class is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class Stack_Ensemble_Classifier (BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        \n",
    "        return None \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.clf1_, self.clf2_ , self.clf3_, self.clf4_,self.clf5_,self.blend_clf_= build_stack_ensemble(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        check_is_fitted(self, ['clf1_', 'clf2_', 'clf3_', 'clf4_', 'clf5_','blend_clf_'])\n",
    "        X = check_array(X)\n",
    "        \n",
    "        new_feature_matrix=np.full((X.shape[0],5), 0.0)\n",
    "        i=0\n",
    "        for clf in (self.clf1_, self.clf2_ , self.clf3_, self.clf4_,self.clf5_):\n",
    "            new_feature_matrix[:,i]=clf.predict(X)\n",
    "            i+=1\n",
    "        \n",
    "        return self.blend_clf_.predict(new_feature_matrix)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        f = self.decision_function(X)\n",
    "        return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-fold cross validation is used to measure performance of the stacked classifier. The results show that the AUC for this classifier is about 51% which is weird and shows that the model is not performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf Stack_Ensemble_Classifier() Scores: AUC stk_ensemble_Mean: 0.51 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "stk_ensemble_clf = Stack_Ensemble_Classifier()\n",
    "\n",
    "roc_scores = cross_val_score(stk_ensemble_clf, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "    \n",
    "print(\"clf\", stk_ensemble_clf, \"Scores:\", \"AUC\",\"stk_ensemble_Mean: %0.2f (+/- %0.2f)\" % (roc_scores.mean(), roc_scores.std() * 2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "Choose a single model based on all previous project steps. Train this model on the complete training dataset and measure it's performance on the held out test set.\n",
    "\n",
    "Compare to the 10-fold CV estimate you got previously.\n",
    "\n",
    "We use AdaBoost model and random forest mdel as the best models. Train them on the whole train set first and then we use them on the test set to see how they are performing. It shows the AUC score is 0.51 and 0.56 respectively and Accuracy is 0.80 and 0.78 respectively. The AUC is much less than what we got from 10-fold CV previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505566542108\n"
     ]
    }
   ],
   "source": [
    "# final result goes here\n",
    "ada_clf_fit=ada_clf.fit(train_X,train_y)\n",
    "y_pred=ada_clf_fit.predict(test_X)\n",
    "AUC_score=roc_auc_score(test_y,y_pred)\n",
    "print(AUC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79665\n"
     ]
    }
   ],
   "source": [
    "Accuracy_score=accuracy_score(test_y,y_pred)\n",
    "print(Accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563674536832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_clf = RandomForestClassifier(max_features= 18, n_estimators= 100)\n",
    "RF_clf_fit=RF_clf.fit(train_X,train_y)\n",
    "y_pred=RF_clf_fit.predict(test_X)\n",
    "AUC_score=roc_auc_score(test_y,y_pred)\n",
    "print(AUC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7797\n"
     ]
    }
   ],
   "source": [
    "Accuracy_score=accuracy_score(test_y,y_pred)\n",
    "print(Accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1994"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4038"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
